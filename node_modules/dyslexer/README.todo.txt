dyslexer
========


[![Bitdeli Badge](https://d2weczhvl823v0.cloudfront.net/rhalff/dyslexer/trend.png)](https://bitdeli.com/free "Bitdeli Badge")

Dyslexer is a simple lexer to be able to parse custom DSL's

The lexer makes use of scopes, by itself dyslexer will just try to read each and every character, but will not understand any of it.

Dyslexer will start to act ones it recognizes a character within it's current scope.

The default scope is the root scope.

### No Scope
```
  'data' -> IN MyProcess(some/component) OUT -> X OtherProcess)
```

Input will just be the output, the dyslexer did not interpret anything


### Root Scope

+-----------------------------------------------------------------+
+  'data' -> IN MyProcess(some/component) OUT -> X OtherProcess)  +
+-----------------------------------------------------------------+

The rootscope is mainly there to delegate to other scopes.
It will make use of it's matchers to do so.
For instance the rootscope must know what to do when the first character
Is encountered, in this case `'`.

The 'data' part indicates data input for this particular DSL. So logically
we have recognized and will define a data scope.

+----------------------------------------------------------------------+
+ +-----------+                                                        +
* | DataScope +                                                        +
+ |           +                                                        +
+ |  'data'   + -> IN MyProcess(some/component) OUT -> X OtherProcess) +
+ +-----------+                                                        +
*                                                                      +
+----------------------------------------------------------------------+

So then we have learned the rootscope about data scope and isolated datascope,
which then can be re-used, might we have to jump to data scope from other scopes.

Jumping into another scope is one part, knowing how to exit this scope is the job of the particular scope itself.
In the case of datascope, we will exit once we have found an (unescaped) matched quote.

The job of each scope is to detect tokens and present them to the dyslexer.

The DataScope will then be left and we are back at the rootScope.

+----------------------------------------------------------------------+
+ +-----------+                                                        +
* | DataScope +                                                        +
+ |           +                                                        +
+ |  'data'   + -> IN MyProcess(some/component) OUT -> X OtherProcess) +
+ +-----------+                                                        +
*                                                                      +
+----------------------------------------------------------------------+

TODO: the api can be simplified much more.

# Rules
Rules are scope functions which run during a character match.

Some scopes do not use rules at all, these scopes are normally just
relying on the end of token, so they can determine which token it is which was matched.

Sometimes however a scope wants to intercept the normal matching.
By default tokens are everything seperated by whitespace.

What scopers use:
 - the char itself
 - the scoper of the scope. (Who brought us into this scope)
 - fireToken()
 - used to determine nesting, I must say in that case rules are unnecessary
   because they.. yep. they could just always direct to a nested scope.
   which probably will clean things up a lot, the thing I now do as a function
   could just automatically jump to the other scope.
   Scope could then be called 'nesting' for example and just leave when nesting is done.
 - Scope jumping.
 - Conditional Scope jumping.

The conditional scope jumping is hard to normalize I guess.
I guess this conditional is the 'real rule', the normal rule is just an exact match.

Some rules are:
- '#' was found so jump to comment scope.
- nesting, how many times did we see this character, if so go to `that` scope.
- Only if we are at the beginning and we are the first char.

Currently not much else. The second is a bit different, because it
considers 2 characters e.g. '(' and ')'

I want to determine how to do this, without requiring anybody to write custom code.
I want to create an editor.

'#': {
  scope: 'CommentScope'
},
'(': {
  scope: 'rootScope', (go back to)
  // just use iffi maybe, argument is current scope.
  rule: 'Nesting'   // could refer to named rule
},
')': {
  scope: 'rootScope', (go back to)
  // just use iffi maybe, argument is current scope.
  rule: 'Nesting'   // could refer to named rule
}

rules: [
 {
   rule: 'Nesting',
   chars: ['(',')']
 },
 {
   rule: 'Default', // just jump to scope
   chars: ['#'],
   target: 'RootScope' // no target could default to root or parent.
   target: 'parent' // no target could default to root or parent.
 }
]


Would be the easiest as much freedom for the rule itself.
Will have access to the entire scope and even the lexer itself.

So what I could do is first refactor those.

What is also nice to do is the post processing. I now use CASE for this. I guess
that could then be done visual, with fbp. Why? because it's fun, fun, fun.



# Structure matching.

A structure is a set of tokens which a scope expects, a scope can define several structures
which the dyslexer then can consider to be valid.

This is useful for debugging and error checking.

# Don'ts and concerns
It's pretty easy to define too many scopes but also to few. One could make just one scope
trying to handle each and every token encountered, it could very well be that for some DSL's this is sufficient.
Or define a one to one relation of Scope and Token. It depends on the DSL what would be the best set of scopes
to deal with.

## Advanced

Scopes also have access to their parent scope, this will allow for digging much deeper
into the context of the a scope, one could even present tokens in a delayed fashion by doing so.
As in, after you've written some lines not knowing yet what they are about, but given this
token encountered, the previous must have meant this.
The parent.parent stuff && iffi, could still make a rule out of this.
E,g, I am in data scope, but want to know my parent scope, to determine what kind of token I am.

Maybe what I am trying to do is normalizing structure and context.
There is not only data & code.
There is data, and rules & code.

And rules can be defined in many ways.
 - regular expression matching.
 - value matching both exact but also context matching
     e.g. the parent.parent stuff and maybe forward also.

If you imagine that rule behavior being present within streamin json or something.
That stream is not just data, it's actually the logic itself.

Once that is finished, stuff like fbpx is also too complex.
what handleTokens bascially does is filtering the tokens, or preparing
them to a final form. Right now that method looks overly complex.
I guess those could all just be in FBP. which makes it much clearer what happens.

Component:
 output: token.name
         token.value

How could I make a graph containing IN_PORT OUTPUT_PORT DATA, whatever token as component?
How could I make that distribution visible?

The weird thing is, I would have to have ONE output port, but the targets would be different.
A distributing port. How else would one solve this in fbp? I think this process is possible.
But it will not be visible.
One way to solve it is with connection conditions. [token.name === 'IN_PORT']
Very visual also, very self explanatory.

Responsibility wise this is not unlogical to be the job of the actor,
it's just a bit smarter actor. Instead of taking a packet to another component
it can first test whether it matches the correct condition.

Condition based connections.

It is then also possible for the actor to determine whether none of the receivers will
accept the packet. It could for instance also pre-check the type.

Imagine this:

Process out[name === 'rob'] -> in Process

Although, this does not make sense:
Process out -> [name === 'rob'] in Process


# ON TOKEN

Ok, trying to determine what is needed:

covers, rootScope, rightHand, provider, leftHand, index, comment, action
  - TOKEN_POSITION (provider, meta)
  - VALIDATE, TOSCOPE
  - VALIDATE, TOKEN_POSITION, TOSCOPE

So:

 start: '{',
 end: '}',

// designed with in the back of the mind, form generation.
// ok I mixed up match & validate

So does this cover all?
If I also take FBP in the back of my mind, the `to` part
prevents me of making this translatable to components.
It limits the code to One component only. with a large definition.
The graph is invisible and internal, which is a bit of a pity.. :-)
Naturally, TO would ofcourse be a connection. So if I translate this to an api.
The return value... see.. distribution. Neh do not translate this to FBP.
You either go forward to a scope, or backward, to Rootscope jumping is stupid.
Anyway, I think if with a format such as below, making additional changes
become much easier.
I think the ultimate test for dyslexer is the fbp parser definition.

{
 name: 'leftHand',
 match: [
  // one match, just implies the ending is the same.
  { match: ['"'], escape: true }
  { match: ['[', ']'], nesting: true } // tracks nesting.
 ],
 [
   { name: 'DATA', match: '{', to: 'root' },
   { name: 'WHATEVER', pos: 1, match: '{', to: 'root' },
   { name: 'XXX', pos: 2, match: '/bla/', to: 'root' }
   { name: 'XXX', pos: 2, match: '/bla/', to: 'root'  }
   {
     name: 'XXX',
     pos: 2,
     match: '/bla/',
     to: 'root'
   }
 ]
}
Also note how this becomes much stronger then it currently seems
I think match should be required. or maybe only to scope and token.
Ok that's pretty clear I guess. I could sqeeze in the filters.
But I'm not sure that it's even the job of a lexer.
But it is very convenient, getting all data served in the way you
want to use it.
Note: how I can then make a form of the above. I'm not sure
filters belong in such a form. They could though:
filters:
  - trim
  -

Problem:
data: does a conversion to json step
JSON: does a weird thing which is probably not needed.

Function: removes leading whitespace.

this.scoper: mainly used to determine token end.
  e.g. ( expect ) to be the end { expect } etc..
So that can just be defined as tokenEnd.. done.

Ok and 'nesting' is just a default setting.
